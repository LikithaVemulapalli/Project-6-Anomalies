# D(St)reams of Anomalies - The real world does not slow down for bad data

For this project, I have considered Twitter_volume_FB dataset from Kaggle.

Dataset link: https://www.kaggle.com/boltzmannbrain/nab

Steps to execute: 

1. Download the files from the github repository. 
2. Get the Twitter_volume_FB.csv file by unzipping the .zip file. 
3. Place the csv files in datasets folder and place the datasets folder in notebooks folder. The notebooks folder should also have ipynb file as well. 
4. Navigate to terminal and type "jupyter notebook" 
5. Navigate to the folder where the notebook is placed. 
6. From the menu icon cell, click on Run all which will run the whole notebook from the first cell. Verify the results.

The project is all about showing the anomalies in the dataset and how they can be easily identified.

Steps to follow:

1. Set up a data science project structure in a new git repository in your GitHub account
2. Download the benchmark data set from
https://www.kaggle.com/boltzmannbrain/nab or
https://github.com/numenta/NAB/tree/master/data
3. Load the one of the data set into panda data frames
4. Formulate one or two ideas on how feature engineering would help the data set to establish additional value using exploratory data analysis
5. Build one or more anomaly detection models to determine the anomalies using the other columns as features
6. Document your process and results
7. Commit your notebook, source code, visualizations and other supporting files to the git repository in GitHub
